# -*- coding: utf-8 -*-
"""Menu Preprocess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cv-c35TLhKYm3FQ8I7Mco2a2RQgOeEY0
"""

# ============================================
# MENU PDF (calendar poster) -> CSV (menu preprocess.csv)
# ============================================

# 0) deps
try:
    import pdfplumber, pandas, rapidfuzz  # noqa
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "pdfplumber", "pandas", "rapidfuzz"])

# 1) mount drive
IN_COLAB = False
try:
    from google.colab import drive  # type: ignore
    drive.mount("/content/drive")
    IN_COLAB = True
except Exception:
    pass

# 2) PATHS — EDIT THESE IF YOUR FOLDERS DIFFER
BASE = "/content/drive/MyDrive/Capstone Project/Data"
MENU_SRC_DIR = f"{BASE}/Menus/Menus"  # folder with your menu PDFs

PREPROC_BASE   = f"{BASE}/preprocess"
PRE_MENU_DIR   = f"{PREPROC_BASE}/menu-processing/preprocessed-data"
MAP_DIR        = f"{PREPROC_BASE}/menu-processing/mappings"
DERIVED_DIR    = f"{PREPROC_BASE}/menu-processing/derived-data"

# Inputs from earlier steps
POS_CSV   = f"{BASE}/preprocess/pos-processing/preprocessed-data/pdf preprocess.csv"
FPROD_CSV = f"{BASE}/processed/fact_production.csv"  # produced earlier; we only READ it

# Optional editable rules file (pattern,category). Set to None to use built-ins.
CATEGORY_RULES_CSV = f"{BASE}/Menus/Menus/category_rules.csv"  # or None

# Ensure folders exist
import os
for d in (PRE_MENU_DIR, MAP_DIR, DERIVED_DIR):
    os.makedirs(d, exist_ok=True)

# File outputs
MENU_OUT      = f"{PRE_MENU_DIR}/menu preprocess.csv"
MAP_OUT       = f"{MAP_DIR}/menu_to_pos_matches.csv"
LOOKUP_OUT    = f"{MAP_DIR}/menu_pos_lookup.csv"
POS_CAT_OUT   = f"{DERIVED_DIR}/pos_with_category.csv"
CAT_MIX_DAILY = f"{DERIVED_DIR}/category_mix_daily.csv"
COST_VS_CAT   = f"{DERIVED_DIR}/cost_vs_category.csv"
ALC_TOP       = f"{DERIVED_DIR}/alacarte_top_items_by_school.csv"

import re, glob
import pandas as pd
import numpy as np
import pdfplumber
from rapidfuzz import fuzz, process

BOILERPLATE_PATTERNS = [
    r"Menu Key", r"Daily Milk Options", r"This institution is an equal opportunity provider",
    r"Salad Bar", r"Fresh (Fruit|Vegetable)", r"Chilled fruit", r"Dried fruit", r"Condiments",
    r"FOOD AND NUTRITION SERVICES", r"(LUNCH|BREAKFAST) MENU", r"Middle School", r"High School", r"Elementary",
    r"Scan QR Code", r"Revised", r"Printed on Recycled Paper", r"POWER\s*PACKS", r"SALAD\s*BAR"
]
BOILERPLATE_RE = re.compile("|".join(BOILERPLATE_PATTERNS), re.I)
DAY_OF_WEEK_RE = re.compile(r"^\s*(Monday|Tuesday|Wednesday|Thursday|Friday|Weekend)\s*$", re.I)
NUMERIC_DATE_RE = re.compile(r"^\s*\d{1,2}\s*$")

def clean_line(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip(" -•\t")

def want_line(s: str) -> bool:
    if not s or len(s) < 3: return False
    if BOILERPLATE_RE.search(s): return False
    if DAY_OF_WEEK_RE.match(s): return False
    if NUMERIC_DATE_RE.match(s): return False
    return True

def merge_wrapped(lines):
    out = []
    for ln in lines:
        if re.match(r"^(w/|with|&)\b", ln.strip().lower()):
            if out: out[-1] = (out[-1] + " " + ln).strip()
        else:
            out.append(ln)
    return out

def extract_items_from_pdf(path: str) -> list[dict]:
    rows = []
    with pdfplumber.open(path) as pdf:
        for pi, p in enumerate(pdf.pages, start=1):
            txt = p.extract_text() or ""
            raw = [clean_line(x) for x in txt.splitlines()]
            cand = [ln for ln in raw if want_line(ln)]
            cand = [ln for ln in cand if len(ln.split()) >= 2]  # ≥2 words → dish-like
            cand = merge_wrapped(cand)
            seen = set()
            for ln in cand:
                key = ln.lower()
                if key in seen: continue
                seen.add(key)
                rows.append({"Source_File": os.path.basename(path), "Page": pi, "Item_Name": ln})
    return rows

# collect PDFs (non-recursive; change to recursive=True to include subfolders)
pdfs = sorted(glob.glob(os.path.join(MENU_SRC_DIR, "*.pdf")))
if not pdfs:
    raise FileNotFoundError(f"No PDF menus found in: {MENU_SRC_DIR}")

print("PDFs found:", len(pdfs))
for p in pdfs[:10]: print(" -", os.path.basename(p))

all_rows = []
for i, f in enumerate(pdfs, 1):
    print(f"[{i}/{len(pdfs)}] {os.path.basename(f)}")
    all_rows.extend(extract_items_from_pdf(f))

menu_df = pd.DataFrame(all_rows)
if menu_df.empty:
    raise RuntimeError("No items extracted from menus. If these are scanned images, we’d need OCR.")

# global de-dup
menu_df["norm"] = menu_df["Item_Name"].str.replace(r"\s+", " ", regex=True).str.strip().str.lower()
menu_df = menu_df.drop_duplicates(subset=["norm"]).drop(columns=["norm"])

def load_category_rules(path=CATEGORY_RULES_CSV):
    if path and os.path.exists(path):
        rules = pd.read_csv(path)
        return [(re.compile(pat, re.I), cat) for pat, cat in rules[["pattern","category"]].itertuples(index=False)]
    starters = [
        ("pizza", "Pizza"),
        ("quesadilla|taco|nachos|burrito|arepas|pupusa", "Tex-Mex"),
        ("burger|bun|sandwich|sub", "Sandwich/Burger"),
        ("grilled cheese|cheese sticks|mozzarella|marinara|calzone", "Cheese/Grain"),
        ("dumpling|potsticker|orange chicken|lo mein|fried rice", "Asian"),
        ("meatball", "Sandwich/Burger"),
        ("masala|chana|brown rice", "Indian"),
        ("pasta|lasagna|alfredo|roll-up|spaghetti|meatballs", "Pasta"),
        ("tender|drumstick|chicken", "Chicken/Entrée"),
        ("fish|filet", "Seafood"),
        ("veggie|vegan|chickenless|plant", "Vegetarian/Vegan"),
    ]
    return [(re.compile(p, re.I), c) for p, c in starters]

CAT_RULES = load_category_rules()

def guess_category(name: str) -> str:
    low = (name or "").lower()
    for pat_re, cat in CAT_RULES:
        if pat_re.search(low):
            return cat
    return "Uncategorized"

menu_df["Category_Guess"] = menu_df["Item_Name"].apply(guess_category)
menu_df.to_csv(MENU_OUT, index=False)
print(f"\nSaved items → {MENU_OUT} | unique items: {len(menu_df):,}")

pos = pd.read_csv(POS_CSV, dtype=str)
pos["Item_Desc"] = pos.get("Item_Desc", "").fillna("")
pos["Item_Desc_norm"] = pos["Item_Desc"].str.replace(r"\s+", " ", regex=True).str.strip()
pos_uniq = pos[["Item_Code","Item_Desc_norm"]].drop_duplicates().rename(columns={"Item_Desc_norm":"Item_Desc"})

choices = pos_uniq["Item_Desc"].tolist()
rows = []
for _, r in menu_df.iterrows():
    q = r["Item_Name"]
    matches = process.extract(q, choices, scorer=fuzz.token_set_ratio, limit=5)
    for choice, score, idx in matches:
        item_row = pos_uniq.iloc[idx]
        rows.append({
            "Menu_Item": q,
            "Category_Guess": r["Category_Guess"],
            "POS_Item_Desc": choice,
            "Match_Score": score,
            "POS_Item_Code": item_row["Item_Code"]
        })

map_df = pd.DataFrame(rows).sort_values(["Menu_Item","Match_Score"], ascending=[True, False])
map_df.to_csv(MAP_OUT, index=False)
print(f"Saved matches → {MAP_OUT}")

# choose best match per menu item with a threshold
BEST_THRESHOLD = 85
best = (map_df.sort_values(["Menu_Item","Match_Score"], ascending=[True,False])
             .groupby("Menu_Item", as_index=False).first())
best["Use_Match"] = best["Match_Score"] >= BEST_THRESHOLD

# build lookup (menu item -> POS item code + category)
lookup = best[best["Use_Match"]][["Menu_Item","POS_Item_Code","Category_Guess"]].rename(
    columns={"POS_Item_Code":"Item_Code","Category_Guess":"Menu_Category"})
lookup.to_csv(LOOKUP_OUT, index=False)
print(f"Lookup (>= {BEST_THRESHOLD}) → {LOOKUP_OUT} | rows: {len(lookup)}")

pos_en = pos.merge(lookup[["Item_Code","Menu_Category"]], on="Item_Code", how="left")

def rule_category_from_desc(desc: str) -> str:
    for pat_re, cat in CAT_RULES:
        if pat_re.search((desc or "")): return cat
    return "Uncategorized"

pos_en["Rule_Category"] = pos_en["Item_Desc"].apply(rule_category_from_desc)
pos_en["Category_Final"] = np.where(pos_en["Menu_Category"].notna(), pos_en["Menu_Category"], pos_en["Rule_Category"])
pos_en.to_csv(POS_CAT_OUT, index=False)
print(f"POS with Category → {POS_CAT_OUT}")

# 7) Category mix per day/school/meal
pos_en["Sold_Total"] = pd.to_numeric(pos.get("Sold_Total", 0), errors="coerce").fillna(0)
pos_en["Date"] = pd.to_datetime(pos.get("Date"), errors="coerce")
grp_cols = ["School_Name","Date","Meal","Category_Final"]
cat_daily = (pos_en.groupby(grp_cols, as_index=False)["Sold_Total"].sum())
tot_daily = (pos_en.groupby(["School_Name","Date","Meal"], as_index=False)["Sold_Total"].sum()
                   .rename(columns={"Sold_Total":"Sold_Total_All"}))
cat_daily = cat_daily.merge(tot_daily, on=["School_Name","Date","Meal"], how="left")
cat_daily["Category_Share"] = np.where(cat_daily["Sold_Total_All"]>0,
                                       cat_daily["Sold_Total"]/cat_daily["Sold_Total_All"], 0)
cat_daily.to_csv(CAT_MIX_DAILY, index=False)
print(f"Category mix (daily) → {CAT_MIX_DAILY}")

# 8) Join with cost-per-meal (from production fact) for explanation plots
if os.path.exists(FPROD_CSV):
    fprod = pd.read_csv(FPROD_CSV)
    fprod["Date"] = pd.to_datetime(fprod["Date"], errors="coerce")
    joined = (fprod.merge(cat_daily, on=["School_Name","Date","Meal"], how="left"))
    # Pivot top categories to columns for easier charting (limit to 6)
    top_cats = (cat_daily["Category_Final"].value_counts().head(6).index.tolist())
    wide = (joined[joined["Category_Final"].isin(top_cats)]
            .pivot_table(index=["School_Name","Date","Meal","Cost_per_Meal","Served_Total"],
                         columns="Category_Final", values="Category_Share", aggfunc="sum")
            .reset_index().fillna(0.0))
    wide.to_csv(COST_VS_CAT, index=False)
    print(f"Cost vs Category mix → {COST_VS_CAT}")
else:
    print("fact_production.csv not found yet; skip cost-vs-category join.")

pos_alc = pos.copy()
for c in ("ALaCarte_Student","ALaCarte_Adult"):
    pos_alc[c] = pd.to_numeric(pos_alc.get(c, 0), errors="coerce").fillna(0)
pos_alc["ALaCarte_Total"] = pos_alc["ALaCarte_Student"] + pos_alc["ALaCarte_Adult"]
alac_top = (pos_alc.groupby(["School_Name","Item_Code","Item_Desc"], as_index=False)["ALaCarte_Total"].sum()
                  .sort_values(["School_Name","ALaCarte_Total"], ascending=[True, False]))
alac_top.to_csv(ALC_TOP, index=False)
print(f"A-la-carte top items by school → {ALC_TOP}")

print("\nDone. Outputs under preprocess/menu-processing/:")
print(" preprocessed-data/")
print("   -", os.path.basename(MENU_OUT))
print(" mappings/")
print("   -", os.path.basename(MAP_OUT))
print("   -", os.path.basename(LOOKUP_OUT))
print(" derived-data/")
print("   -", os.path.basename(POS_CAT_OUT))
print("   -", os.path.basename(CAT_MIX_DAILY))
print("   -", os.path.basename(COST_VS_CAT))
print("   -", os.path.basename(ALC_TOP))

