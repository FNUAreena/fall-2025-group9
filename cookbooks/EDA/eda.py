# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KuFA0uk2IMnkt41eHNfR8meJ_4ECTTrl
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from pathlib import Path

BASE = Path("/content/drive/MyDrive/Capstone Project/Data/preprocess Data")
COMBINED = BASE / "combined_breakfast_lunch.csv"
OUT = BASE / "eda_final"
OUT.mkdir(parents=True, exist_ok=True)

df = pd.read_csv(COMBINED, low_memory=False)
df.columns = (df.columns
              .str.strip().str.lower()
              .str.replace(r"[^a-z0-9]+","_", regex=True))

date_col   = "date" if "date" in df.columns else next(c for c in df.columns if "date" in c)
site_col   = "school_id" if "school_id" in df.columns else ("site_id" if "site_id" in df.columns else "school_name")
sess_col   = "session"
qty_col    = "served_total"
cost_col   = "subtotal_cost"

df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
df = df[~df[date_col].isna()].copy()
df[sess_col] = df[sess_col].astype(str).str.title().str.strip()
df[site_col] = df[site_col].astype(str).str.strip()
df[qty_col]  = pd.to_numeric(df[qty_col], errors="coerce").fillna(0)

df[cost_col] = (df[cost_col].astype(str)
                .str.replace(r"[\$,]", "", regex=True)
                .str.strip())
df[cost_col] = pd.to_numeric(df[cost_col], errors="coerce").fillna(0)

coverage = {
    "rows": [len(df)],
    "start_date": [df[date_col].min().date()],
    "end_date": [df[date_col].max().date()],
    "sessions": [", ".join(sorted(df[sess_col].unique()))],
    "unique_sites": [df[site_col].nunique()],
}

coverage_df = pd.DataFrame(coverage)

daily = (df.groupby([date_col, site_col, sess_col], dropna=False)
           .agg(total_food_cost=(cost_col, "sum"),
                meals_served   =(qty_col, "sum"))
           .reset_index())

daily["cost_per_meal"] = np.where(daily["meals_served"]>0,
                                  daily["total_food_cost"]/daily["meals_served"],
                                  np.nan)

daily = daily.sort_values([site_col, sess_col, date_col])
daily["dow"]  = daily[date_col].dt.day_name()
daily["week"] = daily[date_col].dt.isocalendar().week.astype(int)
daily["wom"]  = ((daily[date_col].dt.day - 1) // 7 + 1)

daily["c_sma7"] = daily.groupby([site_col, sess_col])["cost_per_meal"] \
                       .transform(lambda s: s.rolling(7, min_periods=3).mean())
daily["c_sd7"]  = daily.groupby([site_col, sess_col])["cost_per_meal"] \
                       .transform(lambda s: s.rolling(7, min_periods=3).std())
daily["c_z"]    = (daily["cost_per_meal"] - daily["c_sma7"]) / daily["c_sd7"]

spikes = daily[(daily["c_z"] >= 2)].copy()
spikes = spikes[[date_col, site_col, sess_col, "cost_per_meal", "c_sma7", "c_sd7", "c_z", "meals_served", "total_food_cost"]]

def trend_slope(y):
    x = np.arange(len(y))
    m = ~np.isnan(y)
    if m.sum() < 3: return np.nan
    x, y = x[m], y[m]
    xm, ym = x.mean(), y.mean()
    denom = ((x - xm)**2).sum()
    return 0.0 if denom==0 else (((x - xm)*(y - ym)).sum()/denom)

site_cost = (daily.groupby([site_col, sess_col])
             .agg(days=("cost_per_meal","count"),
                  median_cost=("cost_per_meal","median"),
                  mean_cost=("cost_per_meal","mean"),
                  std_cost=("cost_per_meal","std"),
                  median_meals=("meals_served","median"),
                  cost_trend=("cost_per_meal", trend_slope),
                  spike_rate=("c_z", lambda s: np.mean(s>=2)))
             .reset_index())
site_cost["cov_cost"] = site_cost["std_cost"] / site_cost["mean_cost"]

import matplotlib.pyplot as plt

# 1) Weighted Average Cost per Meal (All Sites) + 7-day SMA
overall = (daily.assign(wc = daily["cost_per_meal"] * daily["meals_served"])
                 .groupby(date_col)
                 .agg(total_meals=("meals_served","sum"),
                      wc=("wc","sum"))
                 .reset_index())
overall["weighted_avg_cost"] = np.where(overall["total_meals"]>0, overall["wc"]/overall["total_meals"], np.nan)
overall = overall.sort_values(date_col)
overall["sma7"] = overall["weighted_avg_cost"].rolling(7, min_periods=3).mean()

plt.plot(overall[date_col], overall["weighted_avg_cost"], label="Weighted avg cost/meal")
plt.plot(overall[date_col], overall["sma7"], label="7-day SMA")
plt.title("Weighted Average Cost per Meal (All Sites)")
plt.xlabel("Date"); plt.ylabel("USD"); plt.legend()

plt.xticks(rotation=45)

plt.tight_layout(); plt.savefig(OUT / "04_weighted_cost_trend_sma.png", dpi=150)
plt.show()

# 2) Average Cost per Meal by Weekday & Session (Grouped Bar)
wk = (daily.groupby([daily[date_col].dt.day_name(), sess_col])["cost_per_meal"]
           .mean().reset_index())
wk.columns = ["dow","session","mean_cost"]
order = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
wk["dow"] = pd.Categorical(wk["dow"], categories=order, ordered=True)
wk = wk.sort_values(["dow","session"])
pivot_wk = wk.pivot(index="dow", columns="session", values="mean_cost").fillna(0)

ax = pivot_wk.plot(kind="bar", rot=30)
ax.set_title("Average Cost per Meal by Weekday & Session")
ax.set_xlabel("Weekday"); ax.set_ylabel("USD")
fig = ax.get_figure(); fig.tight_layout()
fig.savefig(OUT / "05_weekday_cost_by_session_bar.png", dpi=150)
plt.show()

# 3) Cost per Meal vs Meals Served (per-session OLS fit)
colors = daily["session"].map({"Breakfast":"C0","Lunch":"C1"}).fillna("C2")
plt.figure()
plt.scatter(daily["meals_served"], daily["cost_per_meal"], s=10, c=colors)
plt.title("Cost per Meal vs Meals Served (color = session)")
plt.xlabel("Meals Served"); plt.ylabel("Cost per Meal (USD)")

for sess, sub in daily.dropna(subset=["meals_served","cost_per_meal"]).groupby("session"):
    if len(sub) >= 2:
        x = sub["meals_served"].values; y = sub["cost_per_meal"].values
        A = np.vstack([x, np.ones_like(x)]).T
        slope, intercept = np.linalg.lstsq(A, y, rcond=None)[0]
        xs = np.linspace(x.min(), x.max(), 50)
        plt.plot(xs, slope*xs + intercept, label=f"{sess} fit")
plt.legend()
plt.tight_layout(); plt.savefig(OUT / "06_cost_vs_volume_scatter.png", dpi=150)
plt.show()

# 4) Correlation Heatmap (Pearson, constants auto-dropped; includes wom)
corr_df = daily.copy()
corr_df["dow_idx"] = corr_df[date_col].dt.dayofweek
corr_df["session_num"] = corr_df[sess_col].eq("Lunch").astype(int)

num_cols = ["cost_per_meal","meals_served","total_food_cost","dow_idx","week","wom","session_num"]
X = corr_df[num_cols].copy()
constant_cols = [c for c in X.columns if X[c].nunique(dropna=True) <= 1]
X = X.drop(columns=constant_cols)
corr = X.corr(method="pearson")

plt.figure()
plt.imshow(corr.values)
plt.xticks(range(len(X.columns)), X.columns, rotation=45, ha="right")
plt.yticks(range(len(X.columns)), X.columns)
plt.title("Correlation Map (Pearson) â€” constants dropped")
for i in range(len(X.columns)):
    for j in range(len(X.columns)):
        plt.text(j, i, f"{corr.values[i,j]:.2f}", ha="center", va="center")
plt.tight_layout(); plt.savefig(OUT / "07_correlation_heatmap.png", dpi=150)
plt.show()

site_rank_top = (daily.groupby([site_col, sess_col])["cost_per_meal"]
                 .median().rename("median_cost").reset_index()
                 .sort_values("median_cost", ascending=False)
                 .groupby(sess_col).head(15))
site_rank_bot = (daily.groupby([site_col, sess_col])["cost_per_meal"]
                 .median().rename("median_cost").reset_index()
                 .sort_values("median_cost", ascending=True)
                 .groupby(sess_col).head(15))
site_rank_top.to_csv(OUT / "08_top15_sites_by_median_cost.csv", index=False)
site_rank_bot.to_csv(OUT / "09_bottom15_sites_by_median_cost.csv", index=False)

# High-cost spike days (already computed), but sort & keep strongest
spike_top = (daily[daily["c_z"]>=2]
             .sort_values(["c_z", date_col], ascending=[False, True])
             [[date_col, site_col, sess_col, "cost_per_meal","c_sma7","c_sd7","c_z","meals_served","total_food_cost"]]
             .head(200))

spike_top

df['subtotal_cost'].describe()

